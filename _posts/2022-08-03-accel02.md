---
title: Acceleration in Optimization (Part II)
date: 2022-08-03
tags: 
  - mathematics
  - algorithms
  - acceleration
  - optimization
  - convex analysis
use_math: true
---

**Difficulty**: Advanced Undergraduate Students

In our last post, we introduced a few important optimization concepts. With the understanding that the reader has (at least) a cursory knowledge of these concepts, let's present the main idea behind *acceleration*.

We start by introducing one of Nesterov's classic optimization schemes and then see how acceleration for obtaining $\varepsilon$-solutions can be understood within this scheme. For brevity, let us the notation 

$$\ell_{\psi}(\cdot;\bar{x}):=\psi(\cdot)+\left\langle \nabla\psi(\bar{x}),\cdot-\bar{x}\right\rangle$$ 

to denote the linear approximation of a function $\psi:\mathbb{R}^{n}\mapsto\mathbb{R}$ at a given point $\bar{x}\in{\rm dom}\ \psi$.

## Nesterov's Estimating Function Scheme

Consider the following optimization scheme applied to $({\cal P})$.

<ins>**Estimating Function Scheme for $({\cal P})$**</ins>

-   Choose $x_{0}\in{\rm dom}\ h$ and some affine function $\Gamma_{0}:\mathbb{R}^{n}\mapsto\mathbb{R}$. Set $A_{0}\gets0$.

-   For $k=1,\ldots$

    *  Find $(x_{k},A_{k})\in{\rm dom}\ h\times\mathbb{R}\_{++}$ and affine function $\Gamma_{k}:\mathbb{R}^{n}\mapsto\mathbb{R}$ such that 
    
    $$\begin{aligned}
        ({\cal R}_{k}^{1})\qquad & A_{k}\phi(x_{k})\leq\inf_{u\in\mathbb{R}^{n}}\left\{ A_{k}\left[\Gamma_{k}(u)+h(u)\right]+\frac{1}{2}\|u-x_{0}\|^{2}\right\} ,\\
        ({\cal R}_{k}^{2})\qquad & \Gamma(u)\leq f(u)\quad\forall u\in{\rm dom}\ h.\end{aligned}$$

Clearly, for any optimum $x^{*}$ of $({\cal P})$, the generated iterates of the above scheme satisfy 

$$A_{k}\phi(x_{k})\overset{({\cal R}_{k}^{1})}{\leq}A_{k}\left[\Gamma_{k}(x^{*})+h(x^{*})\right]+\frac{1}{2}\|x^{*}-x_{0}\|^{2}\overset{({\cal R}_{k}^{2})}{\leq}A_{k}\phi(x^{*})+\frac{1}{2}\|x^{*}-x_{0}\|^{2},$$ 

for every $k\geq1$. Re-arranging, we have 

$$(*)\qquad\phi(x_{k})-\phi(x^{*})\leq\frac{1}{2A_{k}}\|x^{*}-x_{0}\|^{2}.$$ 

We now make the following bold claim in view of the above result.

**Claim II.1**. <ins>*All "classical" (or "Nesterov-like") accelerated methods are instances (or minor variants) of the Estimating Function Scheme*</ins> *with*

$$A_{k} \approx \begin{cases}
\frac{k^{2}}{L}, & \mu=0,\\
\frac{1}{L}\left(1+\sqrt{\frac{\mu}{L}}\right)^{k-1}, & \mu>0.
\end{cases}$$ 

*As a consequence, accelerated methods obtain an $\varepsilon$-solution of $({\cal P})$ in $O(T_{\varepsilon})$ iterations, where*

$$T_{\varepsilon}=\begin{cases}
\frac{\sqrt{L}\|x_{0}-x^{*}\|}{\sqrt{\varepsilon}}, & \mu=0,\\
\sqrt{\frac{L}{\mu}}\log\left(\frac{L\|x_{0}-x^{*}\|^{2}}{\varepsilon}\right), & \mu>0.
\end{cases}$$

**Remark II.2.** Bounds such as $(*)$ share some similarities with non-accelerated methods. For example, in the proximal gradient method, which has updates of the form 

$$(\dagger)\qquad x_{k}\gets{\rm argmin}_{u\in\mathbb{R}^{n}}\left\{ \lambda_{k}\left[\ell_{f}(u;x_{k-1})+h(u)\right]+\frac{1}{2}\|u-x_{k-1}\|^{2}\right\} ,$$ 

a similar bound as in $(*)$ can be made in which $A_{k}\approx\sum_{i=1}^{k}\lambda_{i}$ under the assumption that $\lambda_{i}\in(0,2/L)$ for every $i\geq1$.

**Remark II.3.** The function $\Gamma_{k}$ acts like a special support function for $f$ whose approximation of $f$ at $x_{k}$  becomes increasingly more accurate as $A_{k}\to\infty$. Indeed, $\Gamma_{k}$ supports $f$ in the sense that $({\cal R}\_{k}^{2})$ implies $\Gamma_{k}$ minorizes $f$ (i.e., bounds it from below at every point in its domain) while $({\cal R}\_{k}^{1})$ implies that 

$$\begin{aligned}
f(x_{k}) & \leq\Gamma_{k}(x_{k})+\frac{1}{2A_{k}}\|x_{k}-x_{0}\|^{2}\\
 & \leq f(x_{k})+\frac{1}{2A_{k}}\|x_{k}-x_{0}\|^{2}\\
 & \leq f(x_{k})+\frac{1}{A_{k}}\|x_{k}-x^{*}\|^{2}+\frac{1}{A_{k}}\|x^{*}-x^{0}\|^{2},\end{aligned}$$ 
 
in which the term $\\|x_{k}-x^{\*}\\|^{2} / A_{k}+\\|x^{\*}-x^{0}\\|^{2} / A_{k}$ can be made small as $A_{k}\to\infty$.

**Remark II.4.** The previous remark gives some insight into how acceleration can be qualified. Specifically, the rate of acceleration is directly related to the quality of the generated estimate sequence $\\{\Gamma\_{k}\\}\_{k\geq1}$ in terms of its approximation of $f$ on the sequence of points $\\{x_{k}\\}\_{k\ge 1}$. The better the approximation, the faster the algorithm converges. For convex (or strongly convex) functions, we will see that their structure plays an important role in creating good estimate sequences.

**Remark II.4.** In Nesterov's [paper](https://link.springer.com/content/pdf/10.1007/s10107-012-0629-5.pdf), the function $\psi_{k}$ is defined to be $A_{k}\Gamma_{k}+\\|\cdot-x_{0}\\|^{2}/2$ in order to simplify some technical expressions. For this series of posts, we opt to use $\Gamma_{k}$ because it is easier to interpret some of the results relative to $\Gamma_k$.

## Dual Gradient Method

Let us give a simple example of a method that implements the Estimating Function Scheme. Consider the following so-called *dual (proximal) gradient method*.

<ins>**Dual Gradient Method for $({\cal P})$**</ins>

-   Choose $x_{0}\in{\rm dom}\ h$ and define $\Gamma_{0}(\cdot)\equiv0$. Set $A_{0}=0$, $v_{0}=x_{0}$, and $y_{0}=x_{0}$.

-   For $k=1,\ldots$

    *  Set $x_{k}\gets{\rm argmin}\_{0\leq i\leq k-1}\phi(y\_{i}).$

    *  Define the new affine function 
    
    $$\Gamma_{k}(\cdot)\gets\frac{A_{k-1}\Gamma_{k-1}(\cdot)}{A_{k}}+\frac{L^{-1}\left[\ell_{f}(\cdot;v_{k-1})+h(\cdot)\right]}{A_{k}}.$$

    *  Set $A_{k}\gets A_{k-1}+L^{-1}$ and compute $$\begin{aligned}
        v_{k} & \gets{\rm argmin}_{u\in\mathbb{R}^{n}}\left\{ A_{k}\Gamma_{k}(u)+\frac{1}{2}\|u-v_{0}\|^{2}\right\} ,\\
        y_{k} & \gets{\rm argmin}_{u\in\mathbb{R}^{n}}\left\{ \ell_{f}(u;v_{k})+h(u)+\frac{L}{2}\|u-v_{k}\|^{2}\right\} .\end{aligned}$$

For completeness, let us show that the iterates satisfy both $({\cal R}\_{k}^{1})$ and $({\cal R}\_{k}^{2})$. Condition $({\cal R}\_{k}^{2})$ follows immediately from the fact that $\Gamma_k$ is a convex combination of affine minorants.

 for every $k\geq0$ and has an update in $y_{k}$ that is similar to the primal (proximal) gradient method in $(\dagger)$. However, it has the undesirable property that $A\_{k}=1/(Lk)$, meaning that it is an *unaccelerated* method. In the next post, we will look at a better way to construct the estimate sequence $\Gamma\_{k}$ to obtain an *accelerated* method.

**Remark II.5.** The affine estimate function $\Gamma_{k}$ is a balanced convex combination of the first-order approximants $\ell_{f}(\cdot;v_{k-1})+h(\cdot)$ at the points $\\{v_{i}\\}_{i\geq0}$. To achieve acceleration, we will consider an *unbalanced* convex combination of similar approximants which have larger associated values of $A_k$.

**Remark II.6.** Since $\Gamma_{k}$ is affine, the update for $v_{k}$ actually has a closed form solution in terms of $v_{0}$, $A_{k}$, and the intercept and normal of $\Gamma_{k}$.

**Remark II.7.** (*For advanced students*) The naming of dual gradient method as a "dual" method remains a mystery to probably all but Nesterov himself. One interpretation that I have about its name comes the duality theory corresponding to perturbation functions that I wrote about in my previous [set](../duality01/) [of](../duality02) [posts](../duality03). Specifically, if a primal optimization problem can be reformulated as the minimization of a perturbation function $\Phi(x,u)$ over $x$ with $u=0$, then its dual problem is given as the maximization of the conjugate $-\Phi^{\*}(\tilde{x},\tilde{u})$ over $\tilde{u}$ with $x=0$ and 

$$\text{dual}\to\sup_{\tilde{u}}-\Phi^{*}(0,\tilde{u})\leq\inf_{x}\Phi(x,u)\gets\text{primal}.$$ 

Since the dual/conjugate problem is about finding the "best" *support vector* for $\Phi(\cdot,\cdot)$, one can see a correspondence to the dual gradient method in the sense that we are looking for the "best" *support function* $\Gamma_k$ for $f$.
